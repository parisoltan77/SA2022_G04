,Approach,Experiment description,MLCQ dataset-P,MLCQ dataset-R,Original study-P,Original study-R
GC_1,[ATFD] > 2 & [WMC] ≥ 47 & [TCC] < 0.33,"A single open-source project.The authors do not report performance measures. Instead, they select a few top code smells(detected by their approach) and discuss why they constitute code smells.",0.75,0.06,N/A,N/A
GC_2,[WMC] ≥ 47 & [TCC] < 0.3 & [ATFD] > 5,"Three open-source projects.The original developers annotated the smells. The authors inform the precision and recall for each project separately. However, as they have reported the exact number of TP, FP, and FN perproject, we calculated the aggregated precision and recall, which we present here.Annotations are not publicly available.",0.75,0.06,0.5,0.38
GC_3,[NOM] > 15 | [NOF] > 15,"A single open-source project.The authors do not report performance measures. Instead, they perform a case study by measuring the number of bug reports issued for the code smells detected by their approach andshow that the largest code smell candidate has the largest number of filed bug reports.",0.44,0.52,N/A,N/A
GC_4,[CLOC] > 750 | [NOM] + [NOF] > 20,"The authors consider nine JavaScript applications chosen for manual checking as they do not have a large codebase. The authors report a total of 15 God Classes in this codebase.The dataset is manually annotated. However, the authors do not specify if multiple annotators examined each code sample. They do not state the annotator’s expertise or understanding of the code smells.The annotations are not publicly available.",0.39,0.44,0.78,0.94
GC_5,[NOM] + [NOF] > 20,"The dataset consists of 11 manually annotated open-source systems.The dataset is manually annotated.Five annotators examined a single system using Brown’s and Fowler’s books as references. The authors report that their approach achieves 100% recall and 89.6% precision on this single system.The authors only report the precision for the remaining ten systems as it is too costly to calculate recall. As the authors report the precision per system, we report the average precision here. The paper states that the dataset is publicly available; however, we could not access the dataset link from the paper when we performed our experiments.",0.39,0.44,0.89,N/A
GC_6,[LCOM] ≥ 0.725 & [WMC] ≥ 34 & [NOF] ≥ 8 & [NOM] ≥ 14,"The authors examined 12 software systems. The authors compare their approach to the results obtained by the existing tools JDeodorant and JSPiRIT. Additionally, annotators examined two of those systems manually using Fowler’s book as a reference. They do not specify whether multiple annotators analyzed the same code snippets. Here, we report the average precision and recall for those two systems. The dataset is not publicly available.",0.34,0.61,0.75,0.45
GC_7,[NOM] > 20 | [NOF] > 9 | [CLOC] > 750,"The dataset consisted of 323 Java classes. The authors report they tested the used heuristics by using known bad smell source code from the “Refactoring: Improving the Design of existing Code” book. The approach had 100% accuracy for some smells for this source code. However, they do not report the accuracy for the Large Class code smell.",0.39,0.39,N/A,N/A
GC_8,[CLOC] > 100 | [WMC] > 20,The authors present a motivating example in which they apply their approach to a single systems’ source code.,0.37,0.7,N/A,N/A
LM_1,MLOC > 50,"The authors consider nine JavaScript applications chosen for manual checking as they do not have a large codebase. The authors report a total of 25 Long Methods in this codebase. The dataset is manually annotated. However, the authors do not specify if multiple annotators examined each code sample. They do not state the annotator’s expertise or understanding of the code smells. The annotations are not publicly available.",1.0,0.2,1,1
LM_2,MLOC > 30 & VG > 4 & NBD > 3,"The authors examined 12 software systems. The authors compare their approach to the results obtained by the existing tools JDeodorant and JSPiRIT. Additionally, annotators examined two of those systems manually using Fowler’s book as a reference. They do not specify whether multiple annotators analyzed the same code snippets. Here, we report the average precision and recall for those two systems. The dataset is not publicly available.",0.91,0.19,0.29,1
LM_3,MLOC > 50 | VG > 10,The authors present a motivating example in which they apply their approach to a single systems’ source code.,0.89,0.31,N/A,N/A
